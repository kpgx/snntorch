{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec239839",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_7_neuromorphic_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "data_path='/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "num_steps = 100\n",
    "TAU = 5\n",
    "THRESHOLD = 0.8\n",
    "beta = 0.5\n",
    "\n",
    "PATH = \"fcn_snn_mnist_latency_tau_5_thresh_0_8_beta_0_5_num_steps_100.pt\"\n",
    "\n",
    "# Torch Variables\n",
    "dtype = torch.float\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cb645-0227-4290-9e1b-25d6ae7eac87",
   "metadata": {
    "id": "107cb645-0227-4290-9e1b-25d6ae7eac87"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Linear(1024, 1000),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.Linear(1000, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True),\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zPFvlqOGi_uW",
   "metadata": {
    "id": "zPFvlqOGi_uW"
   },
   "outputs": [],
   "source": [
    "# this time, we won't return membrane as we don't need it \n",
    "\n",
    "def forward_pass(net, data):  \n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "      m_batch_size = data.size(1)\n",
    "      spk_out, mem_out = net(data[step].view(m_batch_size, -1))\n",
    "      spk_rec.append(spk_out)\n",
    "  \n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VocYbtD7Vwp7",
   "metadata": {
    "id": "VocYbtD7Vwp7"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_count_loss()\n",
    "# loss_fn = SF.mse_temporal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R4GbPSdTUcUR",
   "metadata": {
    "id": "R4GbPSdTUcUR"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "net.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        data = data.to(device)\n",
    "#         data = spikegen.rate(data, num_steps=num_steps)\n",
    "        data = spikegen.latency(data, num_steps=num_steps, tau=5, threshold=THRESHOLD, clip=True, normalize=True, linear=True)\n",
    "\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "#         net.train()\n",
    "#         print(f'input data size = {data.size()}')\n",
    "#         print(f'input data.view(batch_size, -1) size = {data.view(batch_size, -1).size()}')\n",
    "\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    " \n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets) \n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "#         This will end train√∑ing after 50 iterations by default\n",
    "        if i == num_iters:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16246f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(acc_hist)\n",
    "plt.title(\"Train Set Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50df921",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yp2aTX2_1zFG",
   "metadata": {
    "id": "yp2aTX2_1zFG"
   },
   "outputs": [],
   "source": [
    "def batch_accuracy(data_loader, threshold, net):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "    \n",
    "    data_loader = iter(data_loader)\n",
    "    for data, targets in data_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "        \n",
    "      data = spikegen.latency(data, num_steps=num_steps, tau=5, threshold=threshold, clip=True, normalize=True, linear=True)\n",
    "\n",
    "      spk_rec = forward_pass(net, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "#       print(f'acc {acc}')\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = batch_accuracy(test_loader, 0.4, net)\n",
    "print(f\"The total accuracy on the test set is: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3efc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLAfvj9D2AYd",
   "metadata": {
    "id": "qLAfvj9D2AYd"
   },
   "outputs": [],
   "source": [
    "spk_rec = forward_pass(net, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oTKhuyk22M57",
   "metadata": {
    "id": "oTKhuyk22M57"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "idx = 1\n",
    "\n",
    "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
    "labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
    "print(f\"The target label is: {targets[idx]}\")\n",
    "\n",
    "# plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\path\\\\to\\\\your\\\\ffmpeg.exe'\n",
    "\n",
    "#  Plot spike count histogram\n",
    "anim = splt.spike_count(spk_rec[:, idx].detach().cpu(), fig, ax, labels=labels, \n",
    "                        animate=True, interpolate=1)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "# anim.save(\"spike_bar.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06595f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        self.inputs.append(module_in)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "        self.inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ffcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a data batch of size 1 for simplicity\n",
    "# THRESHOLD = 0.4\n",
    "# PATH = \"snn_mnist_latency_tau_5_thresh_0_4_beta_0_5_num_steps_100.pt\"\n",
    "\n",
    "test_loader_batch_size_one = DataLoader(mnist_test, batch_size=1, shuffle=False)\n",
    "for data, targets in test_loader_batch_size_one:\n",
    "    print(data.size())\n",
    "    data = spikegen.latency(data, num_steps=num_steps, tau=5, threshold=THRESHOLD, clip=True, normalize=True, linear=True)\n",
    "    print(data.size())\n",
    "    print(targets)\n",
    "    break\n",
    "\n",
    "input_data_spikes = torch.count_nonzero(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54726e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model\n",
    "\n",
    "# model_save_path = PATH\n",
    "# net = nn.Sequential(nn.Conv2d(1, 12, 5), #in=1x[32x32] out=12x[28x28]\n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "#                     nn.MaxPool2d(2), #in=12x[28x28] out=12x[14x14]\n",
    "#                     nn.Conv2d(12, 32, 5),#in=12x[14x14] out=32x[10x10]\n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "#                     nn.MaxPool2d(2), #in=32x[10x10] out=32x[5x5]\n",
    "#                     nn.Flatten(),\n",
    "#                     nn.Linear(32*5*5, 10), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "#                     ).to(device)\n",
    "# net.load_state_dict(torch.load(model_save_path))\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae77843",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output = SaveOutput()\n",
    "\n",
    "hook_handles = []\n",
    "\n",
    "for layer in net.modules():\n",
    "    if isinstance(layer, snn.Leaky):\n",
    "        handle = layer.register_forward_hook(save_output)\n",
    "        hook_handles.append(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e710f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output.clear()\n",
    "data = data.to(device)\n",
    "spk_rec = forward_pass(net, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tensor = torch.zeros(spk_rec.size()[1:])\n",
    "for i in range(spk_rec.size()[0]):\n",
    "    added_tensor += spk_rec[i]\n",
    "    \n",
    "added_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(save_output.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "leaky1_none_zero_outputs = 0\n",
    "# leaky2_none_zero_outputs = 0\n",
    "leaky3_none_zero_outputs = 0\n",
    "\n",
    "for i in range(0, len(save_output.outputs), 2):\n",
    "    l1 = save_output.outputs[i]\n",
    "#     l2 = save_output.outputs[i+1]\n",
    "    l3 = save_output.outputs[i+1][0]\n",
    "#     print(\"l1 size\", l1.size())\n",
    "    leaky1_none_zero_outputs += torch.count_nonzero(l1)\n",
    "#     print(\"l1\", l1)\n",
    "#     print(\"l2 size\", l2.size())\n",
    "#     leaky2_none_zero_outputs += torch.count_nonzero(l2)\n",
    "#     print(\"l2\", l2)\n",
    "#     print(\"l3 size\", l3.size())\n",
    "    leaky3_none_zero_outputs += torch.count_nonzero(l3)\n",
    "#     print(\"l3\", l3)\n",
    "    \n",
    "#     if i == 2:\n",
    "#     break\n",
    "\n",
    "print(f'input_data_spikes = {input_data_spikes}')\n",
    "print(f'leaky1_none_zero_outputs {leaky1_none_zero_outputs}')\n",
    "# print(f'leaky2_none_zero_outputs {leaky2_none_zero_outputs}')\n",
    "print(f'leaky3_none_zero_outputs {leaky3_none_zero_outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize Network\n",
    "data_in = data[0]\n",
    "print(\"in\",data_in.size())\n",
    "\n",
    "c1_out = nn.Conv2d(1, 12, 5)(data_in)\n",
    "# print(\"Conv2d\",c1_out.size())\n",
    "\n",
    "l1_out = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)(c1_out)\n",
    "print(\"Leaky\",l1_out.size())\n",
    "\n",
    "p1_out = nn.MaxPool2d(2)(l1_out)\n",
    "# print(\"MaxPool2d\",p1_out.size())\n",
    "\n",
    "c2_out = nn.Conv2d(12, 32, 5)(p1_out)\n",
    "# print(\"Conv2d\",c2_out.size())\n",
    "\n",
    "l2_out = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)(c2_out)\n",
    "print(\"Leaky\",l2_out.size())\n",
    "\n",
    "p2_out = nn.MaxPool2d(2)(l2_out)\n",
    "# print(\"MaxPool2d\",p2_out.size())\n",
    "\n",
    "f1 = nn.Flatten()\n",
    "# print(\"Flatten\",f1_out.size())\n",
    "\n",
    "li1_out = nn.Linear(32*5*5, 10)(f1(p2_out))\n",
    "# print(\"Linear\",li1_out.size())\n",
    "\n",
    "l3_out = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)(li1_out)\n",
    "print(\"Leaky\",l3_out.size())\n",
    "\n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "#                     nn.MaxPool2d(2), #in=12x[28x28] out=12x[14x14]\n",
    "#                     nn.Conv2d(12, 32, 5),#in=12x[14x14] out=32x[10x10]\n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "#                     nn.MaxPool2d(2), #in=32x[10x10] out=32x[5x5]\n",
    "#                     nn.Flatten(),\n",
    "#                     nn.Linear(32*5*5, 10), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "#                     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4db80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Copy of tutorial_5_neuromorphic_datasets.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
