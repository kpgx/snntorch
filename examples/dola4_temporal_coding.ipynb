{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4305a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_hist = []\n",
    "# val_loss_hist = []\n",
    "# counter = 0\n",
    "\n",
    "# # Outer training loop\n",
    "# for epoch in range(100):\n",
    "#     minibatch_counter = 0\n",
    "#     train_batch = iter(training_generator)\n",
    "\n",
    "#     # Minibatch training loop\n",
    "#     for data_it, targets_it in train_batch:\n",
    "#         data_it = data_it.to(device)\n",
    "#         targets_it = targets_it.to(device)\n",
    "#         batch_size = data_it.shape[0]\n",
    "#         # Spike generator\n",
    "#         spike_data, spike_targets = spikegen.latency(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps, tau=5, threshold=0.01,\n",
    "#                                                      linear=True, normalize=True, clip=True)\n",
    "        \n",
    "#         output, mem_rec = net(spike_data.view(num_steps, batch_size, 95, 32, 32))\n",
    "#         log_p_y = log_softmax_fn(mem_rec)\n",
    "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "#         # Sum loss over time steps to perform BPTT\n",
    "#         for step in range(num_steps):\n",
    "#             loss_val += loss_fn(log_p_y[step], targets_it)\n",
    "\n",
    "#         # Gradient calculation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward(retain_graph=True)\n",
    "#         nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        \n",
    "#         # Weight Update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "#         loss_hist.append(loss_val.item())\n",
    "\n",
    "#         # Val set\n",
    "#         val_data = itertools.cycle(val_generator)\n",
    "#         valdata_it, valtargets_it = next(val_data)\n",
    "#         valdata_it = valdata_it.to(device)\n",
    "#         valtargets_it = valtargets_it.to(device)\n",
    "#         batch_size = valdata_it.shape[0]\n",
    "#         # Val set spike conversion\n",
    "#         val_spike_data, val_spike_targets = spikegen.latency(valdata_it, valtargets_it, num_outputs=num_outputs, num_steps=num_steps, tau=5, threshold=0.01,\n",
    "#                                                              linear=True, normalize=True, clip=True)\n",
    "        \n",
    "#         # Val set forward pass\n",
    "#         val_output, val_mem_rec = net(val_spike_data.view(num_steps, batch_size, 95, 32, 32))\n",
    "\n",
    "#         # Val set loss\n",
    "#         log_p_yval = log_softmax_fn(val_mem_rec)\n",
    "#         log_p_yval = log_p_yval.sum(dim=0)\n",
    "#         loss_val_val = loss_fn(log_p_yval, valtargets_it)\n",
    "#         val_loss_hist.append(loss_val_val.item())\n",
    "\n",
    "#         # Print val/train loss/accuracy\n",
    "#         if counter % 50 == 0:\n",
    "#               train_printer()\n",
    "#         minibatch_counter += 1\n",
    "#         counter += 1\n",
    "\n",
    "# loss_hist_true_grad = loss_hist\n",
    "# val_loss_hist_true_grad = val_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a2c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_hist = []\n",
    "# val_loss_hist = []\n",
    "# counter = 0\n",
    "\n",
    "# # Outer training loop\n",
    "# for epoch in range(100):\n",
    "#     minibatch_counter = 0\n",
    "#     train_batch = iter(training_generator)\n",
    "\n",
    "#     # Minibatch training loop\n",
    "#     for data_it, targets_it in train_batch:\n",
    "#         data_it = data_it.to(device)\n",
    "#         targets_it = targets_it.to(device)\n",
    "#         batch_size = data_it.shape[0]\n",
    "#         # Spike generator\n",
    "#         spike_data, spike_targets = spikegen.latency(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps, tau=5, threshold=0.01,\n",
    "#                                                      linear=True, normalize=True, clip=True)\n",
    "        \n",
    "#         output, mem_rec = net(spike_data.view(num_steps, batch_size, 95, 32, 32))\n",
    "#         log_p_y = log_softmax_fn(mem_rec)\n",
    "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "#         # Sum loss over time steps to perform BPTT\n",
    "#         for step in range(num_steps):\n",
    "#             loss_val += loss_fn(log_p_y[step], targets_it)\n",
    "\n",
    "#         # Gradient calculation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward(retain_graph=True)\n",
    "#         nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        \n",
    "#         # Weight Update\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "#         loss_hist.append(loss_val.item())\n",
    "\n",
    "#         # Val set\n",
    "#         val_data = itertools.cycle(val_generator)\n",
    "#         valdata_it, valtargets_it = next(val_data)\n",
    "#         valdata_it = valdata_it.to(device)\n",
    "#         valtargets_it = valtargets_it.to(device)\n",
    "#         batch_size = valdata_it.shape[0]\n",
    "#         # Val set spike conversion\n",
    "#         val_spike_data, val_spike_targets = spikegen.latency(valdata_it, valtargets_it, num_outputs=num_outputs, num_steps=num_steps, tau=5, threshold=0.01,\n",
    "#                                                              linear=True, normalize=True, clip=True)\n",
    "        \n",
    "#         # Val set forward pass\n",
    "#         val_output, val_mem_rec = net(val_spike_data.view(num_steps, batch_size, 95, 32, 32))\n",
    "\n",
    "#         # Val set loss\n",
    "#         log_p_yval = log_softmax_fn(val_mem_rec)\n",
    "#         log_p_yval = log_p_yval.sum(dim=0)\n",
    "#         loss_val_val = loss_fn(log_p_yval, valtargets_it)\n",
    "#         val_loss_hist.append(loss_val_val.item())\n",
    "\n",
    "#         # Print val/train loss/accuracy\n",
    "#         if counter % 50 == 0:\n",
    "#               train_printer()\n",
    "#         minibatch_counter += 1\n",
    "#         counter += 1\n",
    "\n",
    "# loss_hist_true_grad = loss_hist\n",
    "# val_loss_hist_true_grad = val_loss_hist\n",
    "\n",
    "\n",
    "# ########\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# targets_mem = spikegen.targets_rate(targets, num_classes=10, on_target=1.05, off_target=0.25)  # returns one-hot encoding where incorrect classes are 0.25, correct classes are 1.05\n",
    "# # You could of course leave on_target and off_target to default values for a simple one-hot-encoding, but I like to encourage the membrane to overshoot the threshold by a little bit to guarantee firing, and I like to ensure incorrect classes are semi-ready to fire.\n",
    "\n",
    "# # feedforward pass\n",
    "# spk_rec, mem_rec = net(data_it.view(batch_size, -1))\n",
    "\n",
    "# # initialize loss to 0\n",
    "# loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# # accumulate MSE loss for all time steps\n",
    "# for step in range(num_steps):\n",
    "#     loss_val += loss_fn(mem_rec[step], targets_mem)\n",
    "\n",
    "# loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e03d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a5f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size=1\n",
    "data_path='/data/mnist'\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "\n",
    "# Torch Variables\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7877a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96913335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of mnist_train is 6000\n"
     ]
    }
   ],
   "source": [
    "from snntorch import utils\n",
    "\n",
    "subset = 10\n",
    "mnist_train = utils.data_subset(mnist_train, subset)\n",
    "print(f\"The size of mnist_train is {len(mnist_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9178f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac03ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0789b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# neuron and simulation parameters\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.5\n",
    "\n",
    "#  Initialize Network\n",
    "net = nn.Sequential(nn.Conv2d(1, 12, 5),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(12, 32, 5),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                    nn.MaxPool2d(2),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(32*4*4, 10),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d88e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, data):  \n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(data.size(0)):  # data.size(0) = number of time steps\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "  \n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c289ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-2, betas=(0.9, 0.999))\n",
    "loss_fn = SF.mse_temporal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "944afc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import spikegen\n",
    "\n",
    "# Iterate through minibatches\n",
    "# data = iter(train_loader)\n",
    "# data_it, targets_it = next(data)\n",
    "# spike_data = spikegen.latency(data_it, num_steps=100, tau=5, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607f3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 0.98 tensor(0.9801, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Accuracy: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_iters = 10\n",
    "num_outputs = 10\n",
    "num_steps=100\n",
    "\n",
    "# tempory hack\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "#         data = spikegen.latency(data, num_steps=100, tau=5, threshold=0.01)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        data = spikegen.latency(data, num_steps=num_steps, tau=5, threshold=0.01,\n",
    "                                                     linear=True, normalize=True, clip=True)\n",
    "        \n",
    "#         print(\"data in\", list(spike_data[:, 0].view(num_steps, -1)))\n",
    "#         print(\"targets\", targets.size(), targets)\n",
    "        \n",
    "#         fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         splt.raster(data[:, 0].view(num_steps, -1), ax, s=25, c=\"black\")\n",
    "\n",
    "#         plt.title(\"Input Layer\")\n",
    "#         plt.xlabel(\"Time step\")\n",
    "#         plt.ylabel(\"Neuron Number\")\n",
    "#         plt.show()\n",
    "        \n",
    "\n",
    "        net.train()\n",
    "        spk_rec = forward_pass(net, data)\n",
    "        \n",
    "#         print(\"data out\", spk_rec)\n",
    "        \n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "#         print(loss_val)\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "    \n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\", loss_val)\n",
    "\n",
    "        acc = SF.accuracy_temporal(spk_rec, targets) \n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # This will end training after 50 iterations by default\n",
    "        if i == num_iters:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a72179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
